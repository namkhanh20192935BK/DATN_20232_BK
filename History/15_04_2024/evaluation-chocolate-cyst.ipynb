{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8121759,"sourceType":"datasetVersion","datasetId":4799113}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":281.30597,"end_time":"2024-04-10T18:11:44.668229","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-10T18:07:03.362259","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0d5cf8473f6f47cd9248f7aa07bffb49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e1118b3c3e443cf8becaeb094b61f09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23716e5e6b374de0a47a518998d00d9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd83c92919aa4c10b4e3effdfa12338b","placeholder":"​","style":"IPY_MODEL_c9b13094f46f4ee7b84d8c98a6cca090","value":"pytorch_model.bin: 100%"}},"3a5e82066b424126b8599179ad075f16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4be448f21139408e89e5411c7560b47e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f0c59cb1e204527a68f4245cde50e64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbb853c615ca47479f06d26c5fc9cc65","max":466,"min":0,"orientation":"horizontal","style":"IPY_MODEL_738d7cf3a07f4af587b5082728ae0bce","value":466}},"56917840741143a198dcc6ff502d7166":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_feccd0bff42a436cb38edfe6f8033ac5","max":6517,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e1118b3c3e443cf8becaeb094b61f09","value":6517}},"597679f701bc494694b4a0a8cba507f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9ba07170ee94de3956c5ee38ccb57db","placeholder":"​","style":"IPY_MODEL_3a5e82066b424126b8599179ad075f16","value":"preprocessor_config.json: 100%"}},"5b33f2ba796a45bd93913744961e0949":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e9b52d4ed64418a9f665fa8c71745fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_597679f701bc494694b4a0a8cba507f1","IPY_MODEL_4f0c59cb1e204527a68f4245cde50e64","IPY_MODEL_6593f3124ba74ce0b3807b4876706198"],"layout":"IPY_MODEL_88b153bba683410c841b45080a426690"}},"5ecbbb6195364db7b8b3c5b7dad6dd7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adf742e91fa24e5d977d1e33bd2b32f8","placeholder":"​","style":"IPY_MODEL_6337e80c5c4f4a5889932d1d6a228ac0","value":" 6.52k/6.52k [00:00&lt;00:00, 593kB/s]"}},"6031bf4ab7434fd6a7afec58c1b06274":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4be448f21139408e89e5411c7560b47e","placeholder":"​","style":"IPY_MODEL_5b33f2ba796a45bd93913744961e0949","value":" 375M/375M [00:01&lt;00:00, 310MB/s]"}},"6337e80c5c4f4a5889932d1d6a228ac0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64f496271ffc4abfbac079f0f54338f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6593f3124ba74ce0b3807b4876706198":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee21dc9eb6274d1ba9bad9f1fe661d8b","placeholder":"​","style":"IPY_MODEL_0d5cf8473f6f47cd9248f7aa07bffb49","value":" 466/466 [00:00&lt;00:00, 38.9kB/s]"}},"738d7cf3a07f4af587b5082728ae0bce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f09b77f51294a4582bba1acee76450f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83a75a78937f4b6c9cbd7d5c85192a48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87881df61bed4598bbd1e366283aeda4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88b153bba683410c841b45080a426690":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a84fa3bb483424586417f3add00ed11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23716e5e6b374de0a47a518998d00d9e","IPY_MODEL_ba6378f893764407803fc8cf1ce270a8","IPY_MODEL_6031bf4ab7434fd6a7afec58c1b06274"],"layout":"IPY_MODEL_64f496271ffc4abfbac079f0f54338f5"}},"a1bbf8549e56432aa163469fe6ef8a76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f09b77f51294a4582bba1acee76450f","placeholder":"​","style":"IPY_MODEL_e773f71c72854116827c029f14435bc6","value":"config.json: 100%"}},"adf742e91fa24e5d977d1e33bd2b32f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba6378f893764407803fc8cf1ce270a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87881df61bed4598bbd1e366283aeda4","max":375045749,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83a75a78937f4b6c9cbd7d5c85192a48","value":375045749}},"c9b13094f46f4ee7b84d8c98a6cca090":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd83c92919aa4c10b4e3effdfa12338b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e773f71c72854116827c029f14435bc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9ba07170ee94de3956c5ee38ccb57db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee21dc9eb6274d1ba9bad9f1fe661d8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef30037b03d744b2a43b03309701f684":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbb853c615ca47479f06d26c5fc9cc65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fecb66f478d94d9a8dd0061e691b0fc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1bbf8549e56432aa163469fe6ef8a76","IPY_MODEL_56917840741143a198dcc6ff502d7166","IPY_MODEL_5ecbbb6195364db7b8b3c5b7dad6dd7d"],"layout":"IPY_MODEL_ef30037b03d744b2a43b03309701f684"}},"feccd0bff42a436cb38edfe6f8033ac5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cài đặt và import các thư viện cần thiết","metadata":{}},{"cell_type":"code","source":"# Install the required libraries\n#SAM\n!pip install git+https://github.com/facebookresearch/segment-anything.git\n#Transformers\n!pip install -q git+https://github.com/huggingface/transformers.git\n#Datasets to prepare data and monai if you want to use special loss functions\n!pip install datasets\n!pip install -q monai\n#Patchify to divide large images into smaller patches for training. (Not necessary for smaller images)\n!pip install patchify","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tifffile\nimport os\nfrom patchify import patchify  #Only to handle large images\nimport random\nfrom scipy import ndimage\nimport cv2\nfrom datasets import Dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random\nimport torch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chia dataset để training model","metadata":{}},{"cell_type":"markdown","source":"## Chia dataset cho tập train","metadata":{}},{"cell_type":"code","source":"# Đường dẫn tới thư mục chứa các hình ảnh\nimage_folder = \"/kaggle/input/otu2d-8layer/OTU2D_8_layers_splitted/Chocolate_Cyst/train/image\"\n\n# Lấy danh sách các tệp trong thư mục và sắp xếp theo thứ tự tên tăng dần từ A đến Z\nimage_files = sorted(os.listdir(image_folder))\n\n# Khởi tạo một danh sách để chứa các hình ảnh dưới dạng mảng NumPy\nimage_array_list = []\n\n# Lặp qua tất cả các tệp trong thư mục đã sắp xếp\nfor filename in image_files:\n    # Kiểm tra xem tệp có phải là hình ảnh không\n    if filename.endswith(('.JPG')):\n        # Đường dẫn đầy đủ tới hình ảnh\n        img_path = os.path.join(image_folder, filename)\n\n        # Đọc hình ảnh\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (256, 256))\n\n        # Kiểm tra xem hình ảnh có được đọc thành công hay không\n        if img is not None:\n            # Thêm hình ảnh vào danh sách\n            image_array_list.append(img)\n        else:\n            print(f\"Không thể đọc hình ảnh {filename}\")\n\n# Chuyển đổi danh sách hình ảnh thành mảng NumPy\ntrain_images_np = np.array(image_array_list)\n\n# In ra kích thước của mảng hình ảnh\nprint(\"Kích thước của mảng hình ảnh:\", train_images_np.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Đường dẫn tới thư mục chứa các hình ảnh\nimage_folder = \"/kaggle/input/otu2d-8layer/OTU2D_8_layers_splitted/Chocolate_Cyst/train/label\"\n\n# Lấy danh sách các tệp trong thư mục và sắp xếp theo thứ tự tên tăng dần từ A đến Z\nimage_files = sorted(os.listdir(image_folder))\n\n# Khởi tạo một danh sách để chứa các hình ảnh dưới dạng mảng NumPy\nimage_array_list = []\n\n# Lặp qua tất cả các tệp trong thư mục đã được sắp xếp\nfor filename in image_files:\n    # Kiểm tra xem tệp có phải là hình ảnh không\n    if filename.endswith('.PNG'):\n        # Đường dẫn đầy đủ tới hình ảnh\n        img_path = os.path.join(image_folder, filename)\n\n        # Đọc hình ảnh\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (256, 256))\n        img = img / 255.0\n        (thresh, img) = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY)\n\n        # img = np.int32(img)\n\n        # Kiểm tra xem hình ảnh có được đọc thành công hay không\n        if img is not None:\n            # Thêm hình ảnh vào danh sách\n            image_array_list.append(img)\n        else:\n            print(f\"Không thể đọc hình ảnh {filename}\")\n\n# Chuyển đổi danh sách hình ảnh thành mảng NumPy\ntrain_labels_np = np.array(image_array_list)\n\n# In ra kích thước của mảng hình ảnh\nprint(\"Kích thước của mảng mặt nạ:\", train_labels_np.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the NumPy arrays to Pillow images and store them in a dictionary\ndataset_dict = {\n    \"image\": [Image.fromarray(img).convert('RGB') for img in train_images_np],\n    \"label\": [Image.fromarray(mask).convert('I') for mask in train_labels_np],\n}\n\n# Create the dataset using the datasets.Dataset class\ntrain_dataset = Dataset.from_dict(dataset_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Chia dataset cho tập validation","metadata":{}},{"cell_type":"code","source":"# Đường dẫn tới thư mục chứa các hình ảnh\nimage_folder = \"/kaggle/input/otu2d-8layer/OTU2D_8_layers_splitted/Chocolate_Cyst/validation/image\"\n\n# Lấy danh sách các tệp trong thư mục và sắp xếp theo thứ tự tên tăng dần từ A đến Z\nimage_files = sorted(os.listdir(image_folder))\n\n# Khởi tạo một danh sách để chứa các hình ảnh dưới dạng mảng NumPy\nimage_array_list = []\n\n# Lặp qua tất cả các tệp trong thư mục đã sắp xếp\nfor filename in image_files:\n    # Kiểm tra xem tệp có phải là hình ảnh không\n    if filename.endswith(('.JPG')):\n        # Đường dẫn đầy đủ tới hình ảnh\n        img_path = os.path.join(image_folder, filename)\n\n        # Đọc hình ảnh\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (256, 256))\n\n        # Kiểm tra xem hình ảnh có được đọc thành công hay không\n        if img is not None:\n            # Thêm hình ảnh vào danh sách\n            image_array_list.append(img)\n        else:\n            print(f\"Không thể đọc hình ảnh {filename}\")\n\n# Chuyển đổi danh sách hình ảnh thành mảng NumPy\nval_images_np = np.array(image_array_list)\n\n# In ra kích thước của mảng hình ảnh\nprint(\"Kích thước của mảng hình ảnh:\", val_images_np.shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Đường dẫn tới thư mục chứa các hình ảnh\nimage_folder = \"/kaggle/input/otu2d-8layer/OTU2D_8_layers_splitted/Chocolate_Cyst/validation/label\"\n\n# Lấy danh sách các tệp trong thư mục và sắp xếp theo thứ tự tên tăng dần từ A đến Z\nimage_files = sorted(os.listdir(image_folder))\n\n# Khởi tạo một danh sách để chứa các hình ảnh dưới dạng mảng NumPy\nimage_array_list = []\n\n# Lặp qua tất cả các tệp trong thư mục đã được sắp xếp\nfor filename in image_files:\n    # Kiểm tra xem tệp có phải là hình ảnh không\n    if filename.endswith('.PNG'):\n        # Đường dẫn đầy đủ tới hình ảnh\n        img_path = os.path.join(image_folder, filename)\n\n        # Đọc hình ảnh\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (256, 256))\n        img = img / 255.0\n        (thresh, img) = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY)\n\n        # img = np.int32(img)\n\n        # Kiểm tra xem hình ảnh có được đọc thành công hay không\n        if img is not None:\n            # Thêm hình ảnh vào danh sách\n            image_array_list.append(img)\n        else:\n            print(f\"Không thể đọc hình ảnh {filename}\")\n\n# Chuyển đổi danh sách hình ảnh thành mảng NumPy\nval_labels_np = np.array(image_array_list)\n\n# In ra kích thước của mảng hình ảnh\nprint(\"Kích thước của mảng mặt nạ:\", val_labels_np.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the NumPy arrays to Pillow images and store them in a dictionary\ndataset_dict = {\n    \"image\": [Image.fromarray(img).convert('RGB') for img in val_images_np],\n    \"label\": [Image.fromarray(mask).convert('I') for mask in val_labels_np],\n}\n\n# Create the dataset using the datasets.Dataset class\nval_dataset = Dataset.from_dict(dataset_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kiểm tra ảnh và mặt nạ","metadata":{}},{"cell_type":"code","source":"img_num = random.randint(0, train_images_np.shape[0]-1)\nexample = train_dataset[img_num]\nimage = example[\"image\"]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_image = train_dataset[img_num][\"image\"]\nexample_mask = train_dataset[img_num][\"label\"]\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot the first image on the left\naxes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\naxes[0].set_title(\"Image\")\n\n# Plot the second image on the right\naxes[1].imshow(example_mask, cmap='gray')  # Assuming the second image is grayscale\naxes[1].set_title(\"Mask\")\n\n# Hide axis ticks and labels\nfor ax in axes:\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n\n# Display the images side by side\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.2])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\nfig, axes = plt.subplots()\n\naxes.imshow(np.array(image))\nground_truth_seg = np.array(example[\"label\"])\nshow_mask(ground_truth_seg, axes)\naxes.title.set_text(f\"Ground truth mask\")\naxes.axis(\"off\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vẽ bounding boxes cho mặt nạ","metadata":{}},{"cell_type":"code","source":"#Get bounding boxes from mask.\ndef get_bounding_box(ground_truth_map):\n  # get bounding box from mask\n  y_indices, x_indices = np.where(ground_truth_map > 0)\n  x_min, x_max = np.min(x_indices), np.max(x_indices)\n  y_min, y_max = np.min(y_indices), np.max(y_indices)\n  # add perturbation to bounding box coordinates\n  H, W = ground_truth_map.shape\n  x_min = max(0, x_min - np.random.randint(0, 20))\n  x_max = min(W, x_max + np.random.randint(0, 20))\n  y_min = max(0, y_min - np.random.randint(0, 20))\n  y_max = min(H, y_max + np.random.randint(0, 20))\n  bbox = [x_min, y_min, x_max, y_max]\n\n  return bbox","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"markdown","source":"## Hàm tạo 1 dataset input images and mask","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass SAMDataset(Dataset):\n  \"\"\"\n  This class is used to create a dataset that serves input images and masks.\n  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n  \"\"\"\n  def __init__(self, dataset, processor):\n    self.dataset = dataset\n    self.processor = processor\n\n  def __len__(self):\n    return len(self.dataset)\n\n  def __getitem__(self, idx):\n    item = self.dataset[idx]\n    image = item[\"image\"]\n    ground_truth_mask = np.array(item[\"label\"])\n\n    # get bounding box prompt | vẽ box cho mặt nạ\n    prompt = get_bounding_box(ground_truth_mask)\n\n    # prepare image and prompt for the model | Chuẩn bị mặt nạ và hộp giới hạn\n    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n\n    # remove batch dimension which the processor adds by default | Loại bỏ chiều Batch được thêm vào mặc định\n    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n\n    # add ground truth segmentation | Thêm ground truth để đánh giá việc Segment sau này, đánh giá hiệu suất mô hình\n    inputs[\"ground_truth_mask\"] = ground_truth_mask\n\n    return inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model SAM","metadata":{}},{"cell_type":"markdown","source":"### Xử lý dữ liệu để tương thích với đầu vào Model","metadata":{}},{"cell_type":"code","source":"# Initialize the processor\nfrom transformers import SamProcessor\nprocessor = SamProcessor.from_pretrained(\"wanglab/medsam-vit-base\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the SAMDataseta\ntrain_dataset = SAMDataset(dataset=train_dataset, processor=processor)\nval_dataset = SAMDataset(dataset=val_dataset, processor=processor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = train_dataset[0]\nfor k,v in example.items():\n  print(f'{k}: {v.shape}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = val_dataset[0]\nfor k,v in example.items():\n  print(f'{k}: {v.shape}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataLoader instance for the training dataset\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)\nval_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True, drop_last=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n  print(k,v.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(val_dataloader))\nfor k,v in batch.items():\n  print(k,v.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch[\"ground_truth_mask\"].shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model Pretrained của Segment Anything","metadata":{}},{"cell_type":"code","source":"# Load the model\nfrom transformers import SamModel\nmodel = SamModel.from_pretrained(\"wanglab/medsam-vit-base\")\n\n# make sure we only compute gradients for mask decoder\nfor name, param in model.named_parameters():\n  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n    param.requires_grad_(False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Khởi tạo model với hàm tối ưu là Adam, hàm loss là DiceCELoss","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom monai.losses import DiceLoss, DiceCELoss, DiceFocalLoss\n\n# Khởi tạo optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0)\n\n# Sử dụng DiceCELoss\nseg_loss = DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n\n# Sử dụng DiceFocalLoss\n# seg_loss = DiceFocalLoss(sigmoid=True, gamma=0.25)\n\n# Sử dụng DiceLoss\n# seg_loss = DiceLoss(to_onehot_y=True, softmax=True)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load hàm loss đánh giá model","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom statistics import mean\nimport torch\nfrom torch.nn.functional import threshold, normalize\n\nsmooth=1e-10\n\n# Define functions for calculating evaluation metrics\ndef dice(predicted, target):\n    true_positive = torch.sum(predicted * target)\n    false_negative = torch.sum(target) - true_positive\n    false_positive = torch.sum(predicted) - true_positive\n    return (2. * true_positive + smooth) / (2. *true_positive + false_negative + false_positive + smooth)\n\ndef iou(predicted, target):\n    intersection = torch.sum(predicted * target)\n    union = torch.sum(predicted) + torch.sum(target) - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef recall(predicted, target):\n    true_positive = torch.sum(predicted * target)\n    false_negative = torch.sum(target) - true_positive\n    return (true_positive + smooth) / (true_positive + false_negative + smooth)\n\ndef precision(predicted, target):\n    true_positive = torch.sum(predicted * target)\n    false_positive = torch.sum(predicted) - true_positive\n    return (true_positive + smooth) / (true_positive + false_positive + smooth)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom statistics import mean\nimport torch\nfrom torch.nn.functional import threshold, normalize\nimport os\n\n# Tạo thư mục checkpoint\ncheckpoint_dir = '/kaggle/working/checkpoint_MEDSAM'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Biến để theo dõi loss tốt nhất và trọng số của nó\nbest_loss = float('inf')\nbest_weights = None\n\n# Add list to record loss of train dataset\ntrain_loss_list = []\ntrain_dice_loss = []\ntrain_iou_loss = []\ntrain_precision_loss = []\ntrain_recall_loss = []\n\n# Add list to record loss of validation dataset\nval_loss_list = []\nval_dice_loss = []\nval_iou_loss = []\nval_precision_loss = []\nval_recall_loss = []\n\n# Early stopping parameters\npatience = 15  # Số lượng epochs mà mô hình không cải thiện trước khi dừng sớm\ncounter = 0\n\n# Training loop\nnum_epochs = 50\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\nmodel.train()\nfor epoch in range(num_epochs):\n    correct_predictions = 0\n    total_predictions = 0\n    \n    # Train Model on Train Dataset\n    epoch_losses = []\n    train_dice_scores = []\n    train_iou_scores = []\n    train_recall_scores = []\n    train_precision_scores = []\n    \n    for batch in tqdm(train_dataloader):\n        # forward pass\n        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n                        input_boxes=batch[\"input_boxes\"].to(device),\n                        multimask_output=False)\n\n        # compute loss\n        predicted_masks = outputs.pred_masks.squeeze(1)\n        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n\n        # backward pass (compute gradients of parameters w.r.t. loss)\n        optimizer.zero_grad()\n        loss.backward()\n\n        # optimize\n        optimizer.step()\n        epoch_losses.append(loss.item())\n        \n        predicted_masks_eval = (outputs.pred_masks.squeeze() > 0.5).float() \n        train_dice_scores.append(dice(predicted_masks_eval, ground_truth_masks))\n        train_iou_scores.append(iou(predicted_masks_eval, ground_truth_masks))\n        train_recall_scores.append(recall(predicted_masks_eval, ground_truth_masks))\n        train_precision_scores.append(precision(predicted_masks_eval, ground_truth_masks))\n\n    # Lưu trọng số của mô hình sau mỗi epoch vào thư mục checkpoint\n    if epoch == num_epochs - 1:\n        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'model_epoch_{epoch}_MedSAM.pt'))\n    else:\n        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'model_epoch_final_MedSAM.pt'))\n\n    # Tính loss trung bình và accuracy của epoch hiện tại\n    epoch_loss_mean = mean(epoch_losses)\n    train_loss_list.append(epoch_loss_mean)\n    \n    train_dice = torch.tensor(train_dice_scores).mean().item()\n    train_iou = torch.tensor(train_iou_scores).mean().item()\n    train_recall = torch.tensor(train_recall_scores).mean().item()\n    train_precision = torch.tensor(train_precision_scores).mean().item()\n    \n    train_dice_loss.append(train_dice)\n    train_iou_loss.append(train_iou)\n    train_recall_loss.append(train_recall)\n    train_precision_loss.append(train_precision)\n    \n    #-------------------------------------------------------------------------------------\n    # Đánh giá mô hình trên tập validation\n    validation_losses = []\n    val_dice_scores = []\n    val_iou_scores = []\n    val_recall_scores = []\n    val_precision_scores = []\n    \n    with torch.no_grad():  # Không tính gradient trong quá trình đánh giá\n        for batch in tqdm(val_dataloader):\n            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n                            input_boxes=batch[\"input_boxes\"].to(device),\n                            multimask_output=False)\n            predicted_masks = outputs.pred_masks.squeeze(1)\n            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n            loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n            validation_losses.append(loss.item())\n            \n            predicted_masks_eval = (outputs.pred_masks.squeeze() > 0.5).float() \n            val_dice_scores.append(dice(predicted_masks_eval, ground_truth_masks))\n            val_iou_scores.append(iou(predicted_masks_eval, ground_truth_masks))\n            val_recall_scores.append(recall(predicted_masks_eval, ground_truth_masks))\n            val_precision_scores.append(precision(predicted_masks_eval, ground_truth_masks))\n\n    # Tính loss trung bình trên tập validation\n    validation_loss_mean = mean(validation_losses)\n    val_loss_list.append(validation_loss_mean)\n    \n    val_dice = torch.tensor(val_dice_scores).mean().item()\n    val_iou = torch.tensor(val_iou_scores).mean().item()\n    val_recall = torch.tensor(val_recall_scores).mean().item()\n    val_precision = torch.tensor(val_precision_scores).mean().item()\n    \n    val_dice_loss.append(val_dice)\n    val_iou_loss.append(val_iou)\n    val_recall_loss.append(val_recall)\n    val_precision_loss.append(val_precision)\n    \n    #-------------------------------------------------------------------------------------\n    # In thông tin về epoch, loss của tập train và validation\n    print(f'EPOCH: {epoch}')\n    print(f'Train Mean loss: {epoch_loss_mean:.4f}')\n    print(f'Train Dice: {train_dice:.4f}')\n    print(f'Train IOU: {train_iou:.4f}')\n    print(f'Train Recall: {train_recall:.4f}')\n    print(f'Train Precision: {train_precision:.4f}')\n    print('----------------------')\n    print(f'Validation Mean loss: {validation_loss_mean:.4f}')\n    print(f'Validation Dice: {val_dice:.4f}')\n    print(f'Validation IOU: {val_iou:.4f}')\n    print(f'Validation Recall: {val_recall:.4f}')\n    print(f'Validation Precision: {val_precision:.4f}')\n\n    # Kiểm tra xem loss của epoch hiện tại có là tốt nhất không\n    if validation_loss_mean < best_loss:\n    # Nếu là loss tốt nhất, cập nhật biến best_loss và lưu trọng số tốt nhất\n        best_loss = validation_loss_mean\n        best_weights = model.state_dict()\n        torch.save(best_weights, os.path.join(checkpoint_dir, 'best_model_weights_Chocolate_Cyst.pt'))\n        print(\"Best model weights saved.\")\n    print('---------------------------------------------')\n    \n    #-------------------------------------------------------------------------------------\n    # Kiểm tra early stopping\n    if epoch > 0:  # Bắt đầu kiểm tra early stopping sau epoch đầu tiên\n        if validation_loss_mean >= prev_epoch_loss:\n            counter += 1\n            if counter >= patience:\n                print(f\"Early stopping! No improvement in {patience} epochs.\")\n                break\n        else:\n            counter = 0  # Reset counter\n            prev_epoch_loss = validation_loss_mean  # Lưu loss của epoch hiện tại để so sánh với epoch tiếp theo\n    else:\n        prev_epoch_loss = validation_loss_mean","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dice_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_iou_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_recall_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_precision_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dice_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_iou_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_recall_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_precision_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Draw chart for loss between train and validation dataset","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nloss = np.arange(epoch + 1)\n\n# Biểu đồ mean loss\nplt.plot(loss, train_loss_list, label='Training Loss')\nplt.plot(loss, val_loss_list, label='Validation Loss') \nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Draw chart for train dataset","metadata":{}},{"cell_type":"code","source":"loss = np.arange(epoch + 1)\n\n# Biểu đồ mean loss\nplt.plot(loss, train_dice_loss, label='Dice') \nplt.plot(loss, train_iou_loss, label='IOU') \nplt.plot(loss, train_precision_loss, label='Precision') \nplt.plot(loss, train_recall_loss, label='Recall') \nplt.xlabel('Epoch')\nplt.ylabel('')\nplt.title('Metrics for training dataset')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Draw chart for validation dataset","metadata":{}},{"cell_type":"code","source":"loss = np.arange(epoch + 1)\n\n# Biểu đồ mean loss\nplt.plot(loss, val_dice_loss, label='Dice') \nplt.plot(loss, val_iou_loss, label='IOU') \nplt.plot(loss, val_precision_loss, label='Precision') \nplt.plot(loss, val_recall_loss, label='Recall') \nplt.xlabel('Epoch')\nplt.ylabel('')\nplt.title('Metrics for validation dataset')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Đánh giá trọng số mô hình với tập Validation Dataset (IOU, Precision, Recall, Dice)","metadata":{}},{"cell_type":"code","source":"# Check if GPU is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to the device\nmodel.to(device)\n\n# Tải trọng số từ checkpoint\ncheckpoint_path = \"/kaggle/working/checkpoint_MEDSAM/best_model_weights_Chocolate_Cyst.pt\"\ncheckpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n\n# Load trọng số vào mô hình\nmodel.load_state_dict(checkpoint)\n\n# Đặt mô hình vào chế độ đánh giá\nmodel.eval()\n\n# Tiếp tục quá trình kiểm tra mô hình như đã thực hiện trước đó\ntest_dice_scores = []\ntest_iou_scores = []\ntest_recall_scores = []\ntest_precision_scores = []\n\nwith torch.no_grad():\n    for batch in tqdm(val_dataloader):\n        # Forward pass\n        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n                        input_boxes=batch[\"input_boxes\"].to(device),\n                        multimask_output=False)\n\n        # Compute evaluation metrics\n        predicted_masks = (torch.sigmoid(outputs['pred_masks']).squeeze() > 0.5).float()\n        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n\n        test_dice_scores.append(dice(predicted_masks, ground_truth_masks))\n        test_iou_scores.append(iou(predicted_masks, ground_truth_masks))\n        test_recall_scores.append(recall(predicted_masks, ground_truth_masks))\n        test_precision_scores.append(precision(predicted_masks, ground_truth_masks))\n\n# Print evaluation metrics\nprint(\"\\n\")\nprint(f'Validation Dice: {torch.tensor(test_dice_scores).mean().item():.4f}')\nprint(f'Validation IOU: {torch.tensor(test_iou_scores).mean().item():.4f}')\nprint(f'Validation Recall: {torch.tensor(test_recall_scores).mean().item():.4f}')\nprint(f'Validation Precision: {torch.tensor(test_precision_scores).mean().item():.4f}')","metadata":{},"execution_count":null,"outputs":[]}]}